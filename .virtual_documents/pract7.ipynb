import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag
import numpy as np


doc="""Natural language processing (NLP) is the ability of a computer program to understand human language as it's spoken and written -- referred to as natural language. It's a component of artificial intelligence (AI).

NLP has existed for more than 50 years and has roots in the field of linguistics. It has a variety of real-world applications in numerous fields, including medical research, search engines and business intelligence.

NLP uses either rule-based or machine learning approaches to understand the structure and meaning of text. It plays a role in chatbots, voice assistants, text-based scanning programs, translation applications and enterprise software that aids in business operations, increases productivity and simplifies different processes.

"""


# Tokenization
tokens = word_tokenize(doc)



# POS Tagging
pos_tags = pos_tag(tokens)



# Stop Words Removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]



# Stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]



# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]



# Print Results
print("Sample Document:")
print(doc)


print("\nTokenization:")
print(tokens)


print("\nPOS Tagging:")
print(pos_tags)


print("\nStop Words Removal:")
print(filtered_tokens)


print("\nStemming:")
print(stemmed_tokens)


print("\nLemmatization:")
print(lemmatized_tokens)


# Function to calculate Term Frequency (TF)
def calculate_tf(word, document):
  tokens = word_tokenize(document)
  return tokens.count(word) / len(tokens)


# Function to calculate Document Frequency (assuming a small document collection here)
def calculate_df(word, documents):
  df = 0
  for doc in documents:
    if calculate_tf(word, doc) > 0:
      df += 1
  return df


# Function to calculate TF-IDF
def calculate_tfidf(word, document, documents):
  tf = calculate_tf(word, document)
  df = calculate_df(word, documents)
  idf = 1 + np.log(len(documents) / (df + 1))  # Using NumPy for log (replace with appropriate library if not available)
  return tf * idf


# Document Collection (assuming this is a small collection for demonstration)
documents = [doc]

# TF-IDF Calculation (example for word "language")
word = "in"
tfidf = calculate_tfidf(word, doc, documents)



print("\nTF-IDF for word '{}':".format(word))
print(tfidf)



