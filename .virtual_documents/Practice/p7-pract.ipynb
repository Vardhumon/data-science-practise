


import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag
import numpy as np


doc="""Natural language processing (NLP) is the ability of a computer program to understand human language as it's spoken and written -- referred to as natural language. It's a component of artificial intelligence (AI).

NLP has existed for more than 50 years and has roots in the field of linguistics. It has a variety of real-world applications in numerous fields, including medical research, search engines and business intelligence.

NLP uses either rule-based or machine learning approaches to understand the structure and meaning of text. It plays a role in chatbots, voice assistants, text-based scanning programs, translation applications and enterprise software that aids in business operations, increases productivity and simplifies different processes.

"""


tokens=word_tokenize(doc)


pos=pos_tag(tokens)


stp_words=set(stopwords.words('English'))
filtered_tokens=[word for word in tokens if word.lower() not in stp_words]


stemmer=PorterStemmer()
stemmed_tokens=[stemmer.stem(word) for word in filtered_tokens ]


Lemmatizer=WordNetLemmatizer()
lemmatized_tokens=[Lemmatizer.lemmatize(word) for word in filtered_tokens]


lemmatized_tokens


def calculate_tf(word,document):
    tokens=word_tokenize(document)
    return tokens.count(word)/len(tokens)


def calculate_df(word,documents):
    df=0
    for doc in documents:
        if calculate_tf(word,doc)> 0:
            df+=1
    return df


def tf_idf(word,document,documents):
    tf=calculate_tf(word,document)
    df=calculate_df(word,documents)
    idf=1 + np.log(len(documents)/(df+1))
    return tf*idf


documents=[doc]
word="in"
print("Term Frequency :",calculate_tf(word,doc))
print("Document Frequency :",calculate_df(word,documents))
print("tf-idf : ",tf_idf(word,doc,documents))



